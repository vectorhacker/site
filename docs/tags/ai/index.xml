<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai on Victor&#39;s Blog</title>
    <link>http://localhost:1313/tags/ai/</link>
    <description>Recent content in Ai on Victor&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>Victor Martínez</copyright>
    <lastBuildDate>Sun, 15 Jun 2025 19:39:41 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Beyond Token Limits: Why the Apple LRM Rebuttal Misses the Point</title>
      <link>http://localhost:1313/posts/why-claudes-comment-paper-is-a-poor-rebuttal/</link>
      <pubDate>Sun, 15 Jun 2025 19:39:41 -0400</pubDate>
      <guid>http://localhost:1313/posts/why-claudes-comment-paper-is-a-poor-rebuttal/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Important Note&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;So it turns out the comment paper was made as a joke, but it did have one geunuine concern. The original author talks about how he didn&amp;rsquo;t expect the paper to go viral, but it did. So since the paper was circled around X and other social media outlets and shared among AI enthusiast circles, this original article will remain here, with this note to encourage anyone to dismiss the claims of the comment paper outright on the basis of it being satirical.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Limits of Current AI: A Skeptical Perspective on Autonomous Agents</title>
      <link>http://localhost:1313/posts/limits-of-current-ai/</link>
      <pubDate>Sun, 25 May 2025 21:11:50 -0400</pubDate>
      <guid>http://localhost:1313/posts/limits-of-current-ai/</guid>
      <description>&lt;p&gt;I align with Yann LeCun&amp;rsquo;s camp on this matter. I don&amp;rsquo;t believe that LLMs and machine learning alone will lead us to human-level intelligence. Current models have demonstrated only very limited reasoning capabilities within specific domains—essentially following and recombining similar patterns they&amp;rsquo;ve ingested during training.&lt;/p&gt;&#xA;&lt;p&gt;If you follow computer scientist and AI researcher Subbarao Kambhampati, a professor at the School of Computing and Augmented Intelligence at ASU, he proposes and demonstrates that language models and reasoning models cannot plan effectively even in domains as simple as organizing blocks in a virtual world, let alone in more complex domains.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Software Engineers Are Safe... For Now</title>
      <link>http://localhost:1313/posts/software-engineers-are-safe-for-now/</link>
      <pubDate>Wed, 25 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/software-engineers-are-safe-for-now/</guid>
      <description>&lt;p&gt;Take a look at this search of jobs postings at OpenAI for December 25th, 2024. As of the time of this writing, over 150 results in all, 87 results for the term &amp;ldquo;Software Engineer.&amp;rdquo; This is a few days after the release of their latest frontier model, o3. This tells me that for the foreseeable future, Software Engineering jobs are safe, if not going to continue to be safe from full automation.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
