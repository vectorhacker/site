<!doctype html>
<html lang="en">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="Important Note
So it turns out the comment paper was made as a joke, but it did have one geunuine concern. The original author talks about how he didn&rsquo;t expect the paper to go viral, but it did. So since the paper was circled around X and other social media outlets and shared among AI enthusiast circles, this original article will remain here, with this note to encourage anyone to dismiss the claims of the comment paper outright on the basis of it being satirical.
" />
<meta name="keywords" content="blog, homepage, technology, go, tech, golang, opinion, ai, llm, reasoning" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="http://localhost:1313/posts/why-claudes-comment-paper-is-a-poor-rebuttal/" />


    <title>
        
            Beyond Token Limits: Why the Apple LRM Rebuttal Misses the Point :: Victor&#39;s Blog 
        
    </title>





  <link rel="stylesheet" href="/main.min.07ea7ac7da67e2e153a7dfa2457bc6a19cca824288d175e223fadc579041bc51.css" integrity="sha256-B&#43;p6x9pn4uFTp9&#43;iRXvGoZzKgkKI0XXiI/rcV5BBvFE=" crossorigin="anonymous">





    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="Beyond Token Limits: Why the Apple LRM Rebuttal Misses the Point">
  <meta itemprop="description" content="Important Note
So it turns out the comment paper was made as a joke, but it did have one geunuine concern. The original author talks about how he didnâ€™t expect the paper to go viral, but it did. So since the paper was circled around X and other social media outlets and shared among AI enthusiast circles, this original article will remain here, with this note to encourage anyone to dismiss the claims of the comment paper outright on the basis of it being satirical.">
  <meta itemprop="datePublished" content="2025-06-15T19:39:41-04:00">
  <meta itemprop="dateModified" content="2025-06-15T19:39:41-04:00">
  <meta itemprop="wordCount" content="945">
  <meta itemprop="image" content="http://localhost:1313/">
  <meta itemprop="keywords" content="Ai,Llm,Reasoning">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/">
  <meta name="twitter:title" content="Beyond Token Limits: Why the Apple LRM Rebuttal Misses the Point">
  <meta name="twitter:description" content="Important Note
So it turns out the comment paper was made as a joke, but it did have one geunuine concern. The original author talks about how he didnâ€™t expect the paper to go viral, but it did. So since the paper was circled around X and other social media outlets and shared among AI enthusiast circles, this original article will remain here, with this note to encourage anyone to dismiss the claims of the comment paper outright on the basis of it being satirical.">







    <meta property="article:published_time" content="2025-06-15 19:39:41 -0400 AST" />









    


  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"
></script>
<script>
    MathJax = {
        tex: {
            displayMath: [
                ["\\[", "\\]"],
                ["$$", "$$"],
            ], 
            inlineMath: [["\\(", "\\)"]], 
        },
        loader: {
            load: ["ui/safe"],
        },
    };
</script>
 
    </head>

    
    <body>
        

        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                echo &#34;Hey!&#34;</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/about/">About</a></li><li><a href="/posts">Posts</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
        </span>
    </span>
</header>


            <div class="content">
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        5 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="http://localhost:1313/posts/why-claudes-comment-paper-is-a-poor-rebuttal/">Beyond Token Limits: Why the Apple LRM Rebuttal Misses the Point</a>
      </h1>

      

      

      

      <div class="post-content">
        <p><strong>Important Note</strong></p>
<p><em>So it turns out the comment paper was made as a <a href="https://lawsen.substack.com/p/when-your-joke-paper-goes-viral">joke</a>, but it did have one geunuine concern. The original author talks about how he didn&rsquo;t expect the paper to go viral, but it did. So since the paper was circled around X and other social media outlets and shared among AI enthusiast circles, this original article will remain here, with this note to encourage anyone to dismiss the claims of the comment paper outright on the basis of it being satirical.</em></p>
<p>Recently Apple <a href="https://machinelearning.apple.com/research/illusion-of-thinking">published a paper</a> on LRMs (Large Reasoning Models) and how they found that &ldquo;that LRMs have limitations in exact computation&rdquo; and that &ldquo;they fail to use explicit algorithms and reason inconsistently across puzzles.&rdquo; I would consider this a death blow paper to the current push for using LLMs and LRMs as the basis for AGI. Subbaro Kambhampati and Yann LeCun seem to agree. You could say that the paper <a href="https://garymarcus.substack.com/p/a-knockout-blow-for-llms">knocked out LLMs</a>. More recently, a comment paper showed up on <a href="https://arxiv.org/html/2506.09250v1">Arxiv</a> and shared around X as a rebuttal to Apple&rsquo;s paper. Putting aside the stunt of having Claude Opus as a co-author (yes, I&rsquo;m not kidding), the paper in itself is a poor rebuttal for many reasons which we shall explore, but mainly for missing the entire point of the paper and prior research by AI researchers such as Professor <a href="https://cotopaxi.eas.asu.edu/">Kambhampati</a>.</p>
<h2 id="mathematical-errors">Mathematical Errors</h2>
<p>Firstly the paper makes some <em>key</em> mathematical errors. As <a href="https://x.com/BlackHC">Andreas Kirsch</a> points out on X, it makes the claim that token growth is predicted by the following:</p>
$$
T(N) \approx 5(2^N - 1)^2 + C
$$<p>namely, it predicts a quadratic token growth for solutions of <a href="https://en.wikipedia.org/wiki/Tower_of_Hanoi">Towers of Hanoi</a>. The reality is that the growth of tokens is linear regardless of the number of steps required to solve. In fact, Gemini 2.5 Pro outputs a solution in under 10k tokens for \(n=10\) discs.</p>
<p>See Andreas&rsquo;s post on X:</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Some comment on this note on arXiv:<br><br>1. Unlike <a href="https://twitter.com/scaling01?ref_src=twsrc%5Etfw">@scaling01</a>, this comment claims that the required number of tokens increases in the square of the number of steps. This is not true.<br>The original lower bound estimate was linear because that is the minimum number of tokens that needâ€¦ <a href="https://t.co/8bcSp1s28f">https://t.co/8bcSp1s28f</a> <a href="https://t.co/RcC398ScPp">pic.twitter.com/RcC398ScPp</a></p>&mdash; Andreas Kirsch ðŸ‡ºðŸ‡¦ (@BlackHC) <a href="https://twitter.com/BlackHC/status/1933442363197706658?ref_src=twsrc%5Etfw">June 13, 2025</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<h2 id="confused-between-mechanical-execution-and-reasoning-complexity">Confused Between Mechanical Execution and Reasoning Complexity</h2>
<p>The rebuttal conflates solution length with computational difficulty. As Kirsch points out, different puzzles have vastly different complexity profiles:</p>
<ul>
<li><strong>Tower of Hanoi</strong>: Requires \(2^N-1\) moves, but has branching factor 1 and requires no search - just mechanical execution with &ldquo;trivial \(O(1)\) decision process per move&rdquo;</li>
<li><strong>River Crossing</strong>: Requires ~4N moves, but has branching factor &gt;4 and is NP-hard, requiring complex constraint satisfaction and search</li>
</ul>
<p>This explains why models might execute 100+ Hanoi moves while failing on 5-move River Crossing problems - the latter requires genuine reasoning while the former is mechanical execution.</p>
<h2 id="contradictions">Contradictions</h2>
<p>The rebuttal paper&rsquo;s own data contradicts its thesis. Its own data shows that models can generate long sequences when they choose to, but in the findings of the original Apple paper, it finds that models <em>systematically</em> choose <strong>NOT</strong> to generate longer reasoning traces on harder problems, effectively just giving up. It&rsquo;s not explained why models would self-impose token limits that hurt their performance. In the original study&rsquo;s findings it says &ldquo;Despite operating well below their generation length limits with ample inference budget available, these models fail to take advantage of additional inference compute&rdquo; (Shojaee et al., p.8).</p>
<p>In the rebuttal&rsquo;s experiments it shows that it can &ldquo;Solve Tower of Hanoi with 15 disks. Output a Lua function that prints the solution when called. Results: Very high accuracy across tested models (Claude-3.7-Sonnet, Claude Opus 4, OpenAI o3, Google Gemini 2.5), completing in under 5,000 tokens&rdquo; (p.3) which directly contradicts its original claim that models are fundamentally constrained by token limits, because this proves that models ARE able to solve complex problems efficiently given appropriate output formats.</p>
<p>The claims in this rebuttal paper create a paradox. If models can solve hard problems efficiently, and models recognize when to truncate output, then why do they systematically choose inefficient approaches that lead to failure? This supports the original study&rsquo;s thesis about reasoning limitations, not the constraint thesis put forth in the rebuttal.</p>
<h2 id="completely-missing-the-point-and-narrow-focus">Completely Missing the Point and Narrow Focus</h2>
<p>The main point of Apple&rsquo;s paper was to identify systematic reasoning patterns, not evaluate accuracy. Indeed, they point out that reasoning benchmarks thus far have inherent limitations because of their leakiness and narrow focus on final solutions, rather than on the general process of reasoning.</p>
<blockquote>
<p>Reasoning models initially increase their thinking tokens proportionally with problem complexity. However, upon approaching a critical thresholdâ€”which closely corresponds to their accuracy collapse pointâ€”models counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty&quot; (Shojaee et al., p.8)</p></blockquote>
<p>It instead completely ignores this finding and offers no explanation as to why models would systematically reduce computational effort when faced with harder problems. This suggests fundamental scaling limitations with current LRM architectures.</p>
<p>Further, the rebuttal does not address the Apple paper&rsquo;s complexity regime patterns that it identified consistently across models or even how token limits explain these.</p>
<h2 id="conclusions-and-further-reading">Conclusions and Further Reading</h2>
<p>Concluding, this comment paper fails to address the key findings of the original paper and focuses too narrowly on contradicting elements. It fundamentally misses the point of Apple&rsquo;s paper, and indeed other papers including by Subbaro Kambhampati, and most recently Georgia Tech, which identify <em>fundamental</em> limitations with current LLM architectures.</p>
<p>I suggest reading up further on <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=subbarao&#43;kambhampati&#43;reason&#43;llm&amp;btnG=">Kambhampati&rsquo;s research</a> into LLM reasoning to get a better idea of why LLM/LRMs are fundamentally limited in reasoning ability. Also read <a href="https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and">Gary Marcus&rsquo;s blog</a> on Substack and the Georgia Tech paper <a href="https://arxiv.org/pdf/2506.07936">here</a>.</p>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="http://localhost:1313/tags/ai/">ai</a></span>
        <span class="tag"><a href="http://localhost:1313/tags/llm/">llm</a></span>
        <span class="tag"><a href="http://localhost:1313/tags/reasoning/">reasoning</a></span>
        
    </p>

      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        945 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2025-06-15 19:39 -0400
        

         
          
        
      </p>
    </div>

    

    
      
        <div id="comments">
          <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "vams" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        </div>
      
    

    

    

  </main>
</div>

             <footer class="footer">
    
    
</footer>
 
        </div>

        



<script type="text/javascript" src="/bundle.min.ad54ad97364f77ede35def9096b162bb1f0b3973aa50b080f5e82fa147f6882e2a7200d7535adbf9b51bebf939f1c1ca9bbe6be87530092aca720eac4a226fda.js" integrity="sha512-rVStlzZPd&#43;3jXe&#43;QlrFiux8LOXOqULCA9egvoUf2iC4qcgDXU1rb&#43;bUb6/k58cHKm75r6HUwCSrKcg6sSiJv2g=="></script>




    </body>
</html>
